{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "KVQA1_yes_no.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/David9857/VQA/blob/main/train/KVQA1_yes_no.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6mdYtS7At_V",
        "outputId": "c430f0da-0029-4ae5-a75f-a1383b80267c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvPQbGRGPV-B"
      },
      "source": [
        "!cp /content/drive/MyDrive/files/data_path.zip /content\n",
        "!unzip data_path.zip \"data/jsons/*\" \"data/pic/*\" \"data/ques_embeddings/biobert/*\" \"data/answer_word_frequency.csv\" \"data/answer_word_frequency.xlsx\" -d /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XerKDOjK0Mxg"
      },
      "source": [
        "!cp /content/drive/MyDrive/data/knowledge_embs.zip /content/data\n",
        "!unzip /content/data/knowledge_embs.zip -d /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1Q8rr76JunX",
        "outputId": "5990c106-3de1-4fb5-e9ab-b62f672c65d2"
      },
      "source": [
        "%cd /content/drive/MyDrive/VQACode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/VQACode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ao4x4KOUmjU"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "import pathlib\n",
        "from utils.load_data import DataLoader\n",
        "from utils.evaluation import AnswerEvaluator\n",
        "from utils.training_toolkit import CustomSchedule, loss_function\n",
        "from models.Transformer.transformers import VQATransformer\n",
        "from models.Transformer.masks import create_masks"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2jYj25HUmjb"
      },
      "source": [
        "### Set up arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiZU7OHDUmjc"
      },
      "source": [
        "num_layers=2\n",
        "d_model=512\n",
        "num_heads=8\n",
        "dff=2048\n",
        "maximum_position_encoding=10000\n",
        "EPOCHS = 30\n",
        "batch_size = 64\n",
        "cnn_type = 'resnet'\n",
        "embedding = 'biobert'  # choose from ['w2v', 'bioelmo', 'biobert', 'bluebert', 'large_biobert', 'elmo', 'bert']\n",
        "data_augmentation = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmwSZlDUmjc"
      },
      "source": [
        "####### DO NOT CHANGE VALUES OF THIS BLOCK IF YOU ARE NOT THE DEVELOPER ##########\n",
        "\n",
        "check_point_path = './check_point/transformer/yesno/' + embedding +'/' + cnn_type + '_' + str(num_layers)\n",
        "saving_folder = './yes_no_results/transformer/' + embedding + '/'\n",
        "save_result_path = saving_folder + cnn_type + '_' + str(num_layers) + '.csv'\n",
        "\n",
        "emb_size = 1024\n",
        "pe_output = 3\n",
        "MAX_LENGTH = pe_output\n",
        "if cnn_type == 'inception':\n",
        "    img_shape = [299, 299]\n",
        "    img_padding = tf.TensorShape([299, 299, 3])\n",
        "if cnn_type in ['resnet', 'resnet_v2', 'dense_net', 'vgg19']:\n",
        "    img_shape = None\n",
        "    img_padding = tf.TensorShape([224, 224, 3])\n",
        "\n",
        "if embedding == 'bioelmo':\n",
        "    pe_input = 38\n",
        "elif embedding == 'elmo':\n",
        "    pe_input = 42\n",
        "elif embedding == 'biobert':\n",
        "    pe_input = 72\n",
        "    emb_size = 768\n",
        "elif embedding == 'bluebert':\n",
        "    pe_input = 69\n",
        "elif embedding == 'large_biobert':\n",
        "    pe_input = 60  \n",
        "elif embedding == 'w2v':\n",
        "    pe_input = 48\n",
        "    emb_size = 200\n",
        "elif embedding == 'bert':\n",
        "    pe_input = 72\n",
        "    emb_size = 1024\n",
        "else:\n",
        "    raise TypeError(\"Wrong embedding type\")\n",
        "    \n",
        "if data_augmentation:\n",
        "    aug = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
        "                               tf.keras.layers.experimental.preprocessing.RandomRotation(0.05)])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_zTsRuUmjc"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-AGwzCUmjd"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y38pUvMUmjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465ae3c7-8a87-401a-d8d3-bc1dc64a2fd8"
      },
      "source": [
        "# create train, val, test dataset\n",
        "kn_input = 194\n",
        "\n",
        "data_loader = DataLoader('/content/data', emb_folder=embedding)\n",
        "full_dataset, tokenizer = data_loader.create_dataset('yes_no')\n",
        "vocab_size=len(tokenizer.index_word) + 1\n",
        "Data_SET_SIZE = len(full_dataset)\n",
        "train_size = int(0.52 * Data_SET_SIZE)\n",
        "val_size = int(0.30 * Data_SET_SIZE)\n",
        "test_size = int(0.18 * Data_SET_SIZE)\n",
        "train_set = full_dataset.take(train_size)\n",
        "val_test_ds = full_dataset.skip(train_size)\n",
        "val_set = val_test_ds.take(val_size)\n",
        "test_ds = val_test_ds.skip(val_size)\n",
        "test_set = test_ds.take(test_size)\n",
        "\n",
        "batch_train_set = train_set.padded_batch(batch_size, padded_shapes=((img_padding, tf.TensorShape([pe_input, emb_size]), tf.TensorShape([kn_input, 1024])),\n",
        "                                                                    tf.TensorShape([pe_output]), []), drop_remainder=True)\n",
        "batch_test_set = test_set.padded_batch(1, padded_shapes=((img_padding, tf.TensorShape([pe_input, emb_size]), tf.TensorShape([kn_input, 1024])),\n",
        "                                                         tf.TensorShape([pe_output]), []), drop_remainder=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fenPNKce01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafe9e51-70b9-4198-db34-b1ec0a3ccbfa"
      },
      "source": [
        "# maxl = 0\n",
        "# maxe = 0\n",
        "# for (img_question, tar, q_id) in full_dataset.as_numpy_iterator():\n",
        "#   # print(img_question[0].shape,img_question[1].shape,tar.shape,q_id)\n",
        "#   # print(img_question[2].shape)\n",
        "#   l = img_question[2].shape[0]\n",
        "#   e = img_question[2].shape[1]\n",
        "#   if l > maxl:\n",
        "#     maxl = l\n",
        "#   if e > maxe:\n",
        "#     maxe = e\n",
        "# print(maxl)\n",
        "# print(maxe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194\n",
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBAQ_IUUmjd"
      },
      "source": [
        "#### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF3AFnpdUmjd"
      },
      "source": [
        "### Define Models and Related Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np9eoY4NUmje"
      },
      "source": [
        "transformer = VQATransformer(num_layers, d_model, num_heads, dff, vocab_size, pe_input, pe_output,\n",
        "                          pretrained_cnn_type=cnn_type)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, check_point_path, max_to_keep=5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOt_cmpIUmje"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(img, question, kn, tar):\n",
        "    if data_augmentation:\n",
        "        img = aug(img)\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(question, tar_inp)\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(question, img, kn, tar_inp,\n",
        "                                     True,\n",
        "                                     enc_padding_mask,\n",
        "                                     combined_mask,\n",
        "                                     dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxEsBRJkUmje"
      },
      "source": [
        "def evaluate(question, img, kn):\n",
        "    end_token = tf.constant(tokenizer.texts_to_sequences(['<end>']), tf.int32)\n",
        "    output = dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    for i in range(MAX_LENGTH):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            question, output)\n",
        "        predictions, attention_weights = transformer(question,\n",
        "                                      img,\n",
        "                                      kn,\n",
        "                                      output,\n",
        "                                      False,\n",
        "                                      enc_padding_mask,\n",
        "                                      combined_mask,\n",
        "                                      dec_padding_mask)\n",
        "\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        if predicted_id == end_token:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6M6dU6NUmje"
      },
      "source": [
        "#### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABdoqTc7Umjf"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR0G40CwUmjf"
      },
      "source": [
        "## restore check point \n",
        "# ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "# print(batch_train_set)\n",
        "# EPOCHS = 10"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2ZEhoLUmjg"
      },
      "source": [
        "def eval_result():  \n",
        "  true_answers_list = []\n",
        "  predicted_answers_list = []\n",
        "  ques_id_list = []\n",
        "  # print('Start predicting...')\n",
        "  for (batch, (img_question, target, ques_id)) in enumerate(batch_test_set):\n",
        "      target = target.numpy()\n",
        "      target = target[0]\n",
        "      true_answer = []\n",
        "      for i in target:\n",
        "          if i == 0:\n",
        "              break\n",
        "          else:\n",
        "              true_answer.append(tokenizer.index_word[i])\n",
        "      true_answer = \" \".join(true_answer[1: -1])\n",
        "\n",
        "      prediction, attention = evaluate(img_question[1], img_question[0], img_question[2])\n",
        "      p = prediction.numpy()\n",
        "      predict_answer = [tokenizer.index_word[i] for i in p][1:]\n",
        "      predict_answer = \" \".join(predict_answer)\n",
        "      true_answers_list.append(true_answer)\n",
        "      predicted_answers_list.append(predict_answer)\n",
        "      ques_id_list.append(ques_id)\n",
        "      print(\"predicted answer: \" + str(batch), end='\\r', flush=True)\n",
        "\n",
        "  # save predictions\n",
        "  data = {\"true answer\": true_answers_list, \"predicted answer\": predicted_answers_list, \"ques_id\":ques_id_list}\n",
        "  df = pd.DataFrame(data)\n",
        "  if not pathlib.Path(saving_folder).exists():\n",
        "      pathlib.Path(saving_folder).mkdir(parents=True, exist_ok=True)\n",
        "  name = save_result_path\n",
        "  df.to_csv(name)\n",
        "  # print(\"complete writing\", name)\n",
        "\n",
        "  # show scores\n",
        "  scores = AnswerEvaluator(name).evaluate()\n",
        "  print(scores)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq3ZLMbXUmjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bfa20a-9d2c-41c2-c478-04ec8ff8596c"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    for (batch, (img_question, tar, _)) in enumerate(batch_train_set):\n",
        "        # print(img_question[0].shape, img_question[1].shape, img_question[2].shape)\n",
        "        train_step(img_question[0], img_question[1], img_question[2], tar)\n",
        "        # if batch % 50 == 0:\n",
        "        #     print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "        #         epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                train_loss.result(),\n",
        "                                                train_accuracy.result()))\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
        "      \n",
        "      eval_result()\n",
        "\n",
        "    # print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.8483 Accuracy 0.6868\n",
            "Epoch 2 Loss 0.2148 Accuracy 0.9006\n",
            "Epoch 3 Loss 0.1796 Accuracy 0.9174\n",
            "Epoch 4 Loss 0.1498 Accuracy 0.9337\n",
            "Epoch 5 Loss 0.1096 Accuracy 0.9536\n",
            "Accuracy: 81.82\n",
            "Exact Match: 81.82\n",
            "F1 Score: 81.24\n",
            "BLEU-1: 81.82\n",
            "BLEU-2: 47.24\n",
            "BLEU-3: 30.44\n",
            "BLEU-4: 15.75\n",
            "{'Accuracy': 81.82, 'Exact Match': 81.82, 'F1 Score': 81.24, 'BLEU-1': 81.82, 'BLEU-2': 47.24, 'BLEU-3': 30.44, 'BLEU-4': 15.75}\n",
            "Epoch 6 Loss 0.0878 Accuracy 0.9654\n",
            "Epoch 7 Loss 0.0804 Accuracy 0.9680\n",
            "Epoch 8 Loss 0.0726 Accuracy 0.9722\n",
            "Epoch 9 Loss 0.0652 Accuracy 0.9759\n",
            "Epoch 10 Loss 0.0530 Accuracy 0.9802\n",
            "Accuracy: 83.93\n",
            "Exact Match: 83.93\n",
            "F1 Score: 83.88\n",
            "BLEU-1: 83.93\n",
            "BLEU-2: 48.46\n",
            "BLEU-3: 31.23\n",
            "BLEU-4: 16.15\n",
            "{'Accuracy': 83.93, 'Exact Match': 83.93, 'F1 Score': 83.88, 'BLEU-1': 83.93, 'BLEU-2': 48.46, 'BLEU-3': 31.23, 'BLEU-4': 16.15}\n",
            "Epoch 11 Loss 0.0521 Accuracy 0.9818\n",
            "Epoch 12 Loss 0.0551 Accuracy 0.9793\n",
            "Epoch 13 Loss 0.0524 Accuracy 0.9811\n",
            "Epoch 14 Loss 0.0507 Accuracy 0.9825\n",
            "Epoch 15 Loss 0.0447 Accuracy 0.9843\n",
            "Accuracy: 82.3\n",
            "Exact Match: 82.3\n",
            "F1 Score: 82.3\n",
            "BLEU-1: 82.3\n",
            "BLEU-2: 47.52\n",
            "BLEU-3: 30.62\n",
            "BLEU-4: 15.84\n",
            "{'Accuracy': 82.3, 'Exact Match': 82.3, 'F1 Score': 82.3, 'BLEU-1': 82.3, 'BLEU-2': 47.52, 'BLEU-3': 30.62, 'BLEU-4': 15.84}\n",
            "Epoch 16 Loss 0.0492 Accuracy 0.9827\n",
            "Epoch 17 Loss 0.0552 Accuracy 0.9810\n",
            "Epoch 18 Loss 0.0474 Accuracy 0.9850\n",
            "Epoch 19 Loss 0.0522 Accuracy 0.9824\n",
            "Epoch 20 Loss 0.0671 Accuracy 0.9790\n",
            "Accuracy: 82.33\n",
            "Exact Match: 82.33\n",
            "F1 Score: 82.2\n",
            "BLEU-1: 82.33\n",
            "BLEU-2: 47.54\n",
            "BLEU-3: 30.63\n",
            "BLEU-4: 15.85\n",
            "{'Accuracy': 82.33, 'Exact Match': 82.33, 'F1 Score': 82.2, 'BLEU-1': 82.33, 'BLEU-2': 47.54, 'BLEU-3': 30.63, 'BLEU-4': 15.85}\n",
            "Epoch 21 Loss 0.0480 Accuracy 0.9840\n",
            "Epoch 22 Loss 0.0416 Accuracy 0.9856\n",
            "Epoch 23 Loss 0.0498 Accuracy 0.9814\n",
            "Epoch 24 Loss 0.0446 Accuracy 0.9859\n",
            "Epoch 25 Loss 0.0412 Accuracy 0.9862\n",
            "Accuracy: 84.48\n",
            "Exact Match: 84.48\n",
            "F1 Score: 84.48\n",
            "BLEU-1: 84.48\n",
            "BLEU-2: 48.77\n",
            "BLEU-3: 31.43\n",
            "BLEU-4: 16.26\n",
            "{'Accuracy': 84.48, 'Exact Match': 84.48, 'F1 Score': 84.48, 'BLEU-1': 84.48, 'BLEU-2': 48.77, 'BLEU-3': 31.43, 'BLEU-4': 16.26}\n",
            "Epoch 26 Loss 0.0469 Accuracy 0.9834\n",
            "Epoch 27 Loss 0.0430 Accuracy 0.9854\n",
            "Epoch 28 Loss 0.0449 Accuracy 0.9856\n",
            "Epoch 29 Loss 0.0412 Accuracy 0.9863\n",
            "Epoch 30 Loss 0.0434 Accuracy 0.9857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1o-VT7MUmjg"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je1gO6T6Umjg"
      },
      "source": [
        "### Predicting and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs9ix8d1Umjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dcbf77-19f8-4ce9-dc62-d7dbbdd64c4f"
      },
      "source": [
        "\n",
        "data = {\"true answer\": true_answers_list, \"predicted answer\": predicted_answers_list, \"ques_id\":ques_id_list}\n",
        "df = pd.DataFrame(data)\n",
        "if not pathlib.Path(saving_folder).exists():\n",
        "    pathlib.Path(saving_folder).mkdir(parents=True, exist_ok=True)\n",
        "name = save_result_path\n",
        "df.to_csv(name)\n",
        "print(\"complete writing\", name)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete writing ./yes_no_results/transformer/biobert/resnet_2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTwUPnuNUmjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52308d35-2770-4b1e-cdde-7c7405f4c0f4"
      },
      "source": [
        "scores = AnswerEvaluator(name).evaluate()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 83.59\n",
            "Exact Match: 83.59\n",
            "F1 Score: 83.24\n",
            "BLEU-1: 83.59\n",
            "BLEU-2: 48.26\n",
            "BLEU-3: 31.1\n",
            "BLEU-4: 16.09\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}